{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d880610b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee4bf0b5",
   "metadata": {},
   "source": [
    "Load necessary modules to environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d98b28c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import machine_learning_new as ml\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13cc2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_str = (\n",
    "    r'DRIVER={SQL Server};'\n",
    "    r'SERVER=lct-sqlbidev\\dev;'\n",
    "    r'DATABASE=Informatics_SSAS_Live;'\n",
    "    r'Trusted_Connection=yes;'\n",
    "    )\n",
    "\n",
    "cnxn = pyodbc.connect(conn_str) # connect using the connection string\n",
    "\n",
    "cursor_source = cnxn.cursor()\n",
    "\n",
    "cursor_source.execute(\"EXEC [Informatics_SSAS_Live].[Reporting].\"\n",
    "               \"[usp_ML_Inpatient_Readmissions_process]\") # the sql we want to run\n",
    "\n",
    "source_data = cursor_source.fetchall() # return all the data\n",
    "\n",
    "\n",
    "# get list of headers using list comprehension - this will account for new \n",
    "# columns dynamically as they are added to the SQL source data\n",
    "source_headers = [column[0] for column in cursor_source.description] \n",
    "\n",
    "#headers\n",
    "\n",
    "# load data into pandas dataframe\n",
    "source_df = pd.DataFrame(np.array(source_data),\n",
    "                                columns = source_headers)\n",
    "\n",
    "# source_df['ReAdmission'] = source_df['ReAdmission'].astype(int)\n",
    "\n",
    "#source_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c24dfc7",
   "metadata": {},
   "source": [
    "Truncate categorical data to make results easier to read and handle DQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bb2e1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Converted 'ReAdmission' to numeric\n",
      "[INFO] Converted 'DeprivationIndex' to numeric\n",
      "[INFO] Converted 'LearningDisability' to numeric\n",
      "[INFO] Converted 'AutismDiagnosis' to numeric\n",
      "[INFO] Converted 'ExBAF' to numeric\n",
      "[INFO] Converted 'ethnicity_clean' to category\n",
      "[INFO] Converted 'gender_clean' to category\n",
      "[INFO] Converted 'accom_clean' to category\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\Departmental Shares\\IM&T\\Information\\Business Intelligence\\Heath McDonald\\HSMA\\Machine Learning\\LSCFT_ML_App\\code\\machine_learning_new.py:106: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='raise')\n",
      "s:\\Departmental Shares\\IM&T\\Information\\Business Intelligence\\Heath McDonald\\HSMA\\Machine Learning\\LSCFT_ML_App\\code\\machine_learning_new.py:106: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='raise')\n",
      "s:\\Departmental Shares\\IM&T\\Information\\Business Intelligence\\Heath McDonald\\HSMA\\Machine Learning\\LSCFT_ML_App\\code\\machine_learning_new.py:106: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='raise')\n",
      "s:\\Departmental Shares\\IM&T\\Information\\Business Intelligence\\Heath McDonald\\HSMA\\Machine Learning\\LSCFT_ML_App\\code\\machine_learning_new.py:106: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='raise')\n",
      "s:\\Departmental Shares\\IM&T\\Information\\Business Intelligence\\Heath McDonald\\HSMA\\Machine Learning\\LSCFT_ML_App\\code\\machine_learning_new.py:106: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='raise')\n",
      "s:\\Departmental Shares\\IM&T\\Information\\Business Intelligence\\Heath McDonald\\HSMA\\Machine Learning\\LSCFT_ML_App\\code\\machine_learning_new.py:106: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='raise')\n",
      "s:\\Departmental Shares\\IM&T\\Information\\Business Intelligence\\Heath McDonald\\HSMA\\Machine Learning\\LSCFT_ML_App\\code\\machine_learning_new.py:106: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='raise')\n",
      "s:\\Departmental Shares\\IM&T\\Information\\Business Intelligence\\Heath McDonald\\HSMA\\Machine Learning\\LSCFT_ML_App\\code\\machine_learning_new.py:106: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='raise')\n"
     ]
    }
   ],
   "source": [
    "if 'Ethnicity' in source_df.columns:\n",
    "\n",
    "    conditions_ethnicity = [\n",
    "        source_df['Ethnicity'] == 'Not Known/Specified',\n",
    "        source_df['Ethnicity'] == 'Not stated',\n",
    "        source_df['Ethnicity'] == 'White - British',\n",
    "        source_df['Ethnicity'] == 'White and Black African',\n",
    "        source_df['Ethnicity'] == 'Any other Asian background',\n",
    "        source_df['Ethnicity'] == 'White and Asian',\n",
    "        source_df['Ethnicity'] == 'Pakistani',\n",
    "        source_df['Ethnicity'] == 'Indian',\n",
    "        source_df['Ethnicity'] == 'Bangladeshi',\n",
    "        source_df['Ethnicity'] == 'Any other White background',\n",
    "        source_df['Ethnicity'] == 'Any other mixed background',\n",
    "        source_df['Ethnicity'] == 'Chinese',\n",
    "        source_df['Ethnicity'] == 'Any other ethnic group',\n",
    "        source_df['Ethnicity'] == 'White - Irish',\n",
    "        source_df['Ethnicity'] == 'Black or Black British - Caribbean',\n",
    "        source_df['Ethnicity'] == 'White and Black Caribbean',\n",
    "        source_df['Ethnicity'] == 'Any other Black background',\n",
    "        source_df['Ethnicity'] == 'Black or Black British - African'\n",
    "    ]       \n",
    "\n",
    "    outputs = [\n",
    "        'Not Known', 'NotStated', 'WhiteBr', 'WhiteBlkAfr', 'AsianOther',\n",
    "        'WhiteAsian', 'Pakistani', 'Indian', 'Bangladeshi',\n",
    "        'OtherWhite','OtherMixed','Chinese','AnyOther',\n",
    "        'WhiteIrish','Caribbean','WhtBlkCarib','BlackOther','BlkAfrican' \n",
    "\n",
    "    ]\n",
    "    # add new column \n",
    "    source_df['ethnicity_clean'] = np.select(conditions_ethnicity, outputs, 'Err')\n",
    "    # get rid of old column\n",
    "    source_df.drop('Ethnicity',axis=1,inplace=True)\n",
    "\n",
    "if 'Gender' in source_df.columns:\n",
    "\n",
    "    conditions_gender = [\n",
    "        source_df['Gender'] == 'Male',\n",
    "        source_df['Gender'] == 'Female',\n",
    "        source_df['Gender'] == 'Not Known',\n",
    "        source_df['Gender'] == 'Not Specified'\n",
    "    ]       \n",
    "\n",
    "    outputs_gender = [\n",
    "        'Male', 'Female', 'NK', 'NK'\n",
    "    ]\n",
    "    # add new column \n",
    "    source_df['gender_clean'] = np.select(conditions_gender, outputs_gender\n",
    "                                                , 'Err')\n",
    "    # get rid of old column\n",
    "    source_df.drop('Gender',axis=1,inplace=True)\n",
    "\n",
    "if 'AccommodationStatus' in source_df.columns:\n",
    "\n",
    "    conditions_accom = [\n",
    "        source_df['AccommodationStatus'] == 'Owner occupier',\n",
    "        source_df['AccommodationStatus'] == 'Unknown',\n",
    "        source_df['AccommodationStatus'] == 'Not known',\n",
    "        source_df['AccommodationStatus'] == 'Tenant - private landlord',\n",
    "        source_df['AccommodationStatus'] == 'Mainstream Housing',\n",
    "        source_df['AccommodationStatus'] == 'Tenant - Housing Association',\n",
    "        source_df['AccommodationStatus'] == 'Accommodation with mental health care support',\n",
    "        source_df['AccommodationStatus'] == 'Secure psychiatric unit',\n",
    "        source_df['AccommodationStatus'] == 'Independent hospital/clinic',\n",
    "        source_df['AccommodationStatus'] == 'Sheltered housing for older persons',\n",
    "        source_df['AccommodationStatus'] == 'Other accommodation with mental health care and support',\n",
    "        source_df['AccommodationStatus'] == 'Homeless',\n",
    "        source_df['AccommodationStatus'] == 'Settled mainstream housing with family/friends',\n",
    "        source_df['AccommodationStatus'] == 'NHS acute psychiatric ward',\n",
    "        source_df['AccommodationStatus'] == 'Specialist rehabilitation/recovery',\n",
    "        source_df['AccommodationStatus'] == 'Supported accommodation',\n",
    "        source_df['AccommodationStatus'] == 'Non-Mental Health Registered Care Home',\n",
    "        source_df['AccommodationStatus'] == 'Mental Health Registered Care Home',\n",
    "        source_df['AccommodationStatus'] == '[NOVALUE]',\n",
    "        source_df['AccommodationStatus'] == 'Staying with friends/family as a short term guest',\n",
    "        source_df['AccommodationStatus'] == 'Rough sleeper',\n",
    "        source_df['AccommodationStatus'] == 'Tenant - Local Authority/Arms Length Management Organisation/Registered Landlord',\n",
    "        source_df['AccommodationStatus'] == 'Other NHS facilities/hospital'\n",
    "    ]       \n",
    "\n",
    "    outputs_accom = [\n",
    "        'Owner', 'NK', 'NK', 'Private','Mainstream','HA','Supp','Psych','Hosp',\n",
    "        'Shelt','Supp','HL','FF','Psych','Rehab','Supp','CH','CH','NK','FF'\n",
    "        ,'HL','HA','NHS']\n",
    "    # add new column \n",
    "    source_df['accom_clean'] = np.select(conditions_accom, outputs_accom\n",
    "                                                , 'Oth')\n",
    "    # get rid of old column\n",
    "    source_df.drop('AccommodationStatus',axis=1,inplace=True)\n",
    "\n",
    "source_df.to_csv('check_source_data.csv', index=False)\n",
    "\n",
    "# cols_to_convert_int = ['ReAdmission', 'LearningDisability', 'AutismDiagnosis','ExBAF']\n",
    "# source_df[cols_to_convert_int] = source_df[cols_to_convert_int].astype('int64')\n",
    "\n",
    "#print(source_df.dtypes)\n",
    "\n",
    "# make sure data is of correct data types\n",
    "source_df = ml.fix_dtypes(source_df)\n",
    "\n",
    "#print(source_df.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f858f",
   "metadata": {},
   "source": [
    "split data into X and y values and scale numerical values or one hot encode categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57a9ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ethnicity_clean', 'gender_clean', 'accom_clean']\n",
      "['DeprivationIndex']\n",
      "      DeprivationIndex  LearningDisability  AutismDiagnosis  ExBAF  \\\n",
      "0            -0.174007                   0                0      0   \n",
      "1            -0.381852                   0                0      0   \n",
      "2            -0.312570                   0                0      0   \n",
      "3            -0.312570                   0                0      0   \n",
      "4            -0.312570                   0                0      0   \n",
      "...                ...                 ...              ...    ...   \n",
      "8635          6.407770                   0                0      0   \n",
      "8636          6.407770                   0                0      0   \n",
      "8637          6.407770                   0                0      0   \n",
      "8638          6.407770                   0                0      0   \n",
      "8639          6.407770                   0                0      0   \n",
      "\n",
      "      eth_AnyOther  eth_AsianOther  eth_Bangladeshi  eth_BlackOther  \\\n",
      "0                0               0                0               0   \n",
      "1                0               0                0               0   \n",
      "2                0               0                0               0   \n",
      "3                0               0                0               0   \n",
      "4                0               0                0               0   \n",
      "...            ...             ...              ...             ...   \n",
      "8635             0               0                0               0   \n",
      "8636             0               0                0               0   \n",
      "8637             0               0                0               0   \n",
      "8638             0               0                0               0   \n",
      "8639             0               0                0               0   \n",
      "\n",
      "      eth_BlkAfrican  eth_Caribbean  ...  acc_Mainstream  acc_NHS  acc_NK  \\\n",
      "0                  0              0  ...               0        0       0   \n",
      "1                  0              0  ...               0        0       0   \n",
      "2                  0              0  ...               0        0       1   \n",
      "3                  0              0  ...               0        0       1   \n",
      "4                  0              0  ...               0        0       1   \n",
      "...              ...            ...  ...             ...      ...     ...   \n",
      "8635               0              0  ...               0        0       0   \n",
      "8636               0              0  ...               0        0       1   \n",
      "8637               0              0  ...               0        0       1   \n",
      "8638               0              0  ...               1        0       0   \n",
      "8639               0              0  ...               1        0       0   \n",
      "\n",
      "      acc_Oth  acc_Owner  acc_Private  acc_Psych  acc_Rehab  acc_Shelt  \\\n",
      "0           0          0            0          0          0          0   \n",
      "1           0          0            0          0          0          0   \n",
      "2           0          0            0          0          0          0   \n",
      "3           0          0            0          0          0          0   \n",
      "4           0          0            0          0          0          0   \n",
      "...       ...        ...          ...        ...        ...        ...   \n",
      "8635        0          0            0          0          0          0   \n",
      "8636        0          0            0          0          0          0   \n",
      "8637        0          0            0          0          0          0   \n",
      "8638        0          0            0          0          0          0   \n",
      "8639        0          0            0          0          0          0   \n",
      "\n",
      "      acc_Supp  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "...        ...  \n",
      "8635         0  \n",
      "8636         0  \n",
      "8637         0  \n",
      "8638         0  \n",
      "8639         0  \n",
      "\n",
      "[8640 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "target_column = \"ReAdmission\" # what we're interested in predicting\n",
    "\n",
    "# Separate features and target\n",
    "X = source_df.drop(target_column, axis=1)\n",
    "y = source_df[target_column]\n",
    "\n",
    "# Detect column types\n",
    "categorical_cols = X.select_dtypes(include=['string','object','category']).columns.tolist()\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64', 'uint8']).columns\n",
    "\n",
    "# Identify one-hot encoded columns\n",
    "one_hot_cols = [col for col in numeric_cols if ml.is_one_hot_column(X[col])]\n",
    "\n",
    "# Now exclude them from numerical preprocessing\n",
    "numerical_cols = [col for col in numeric_cols if col not in one_hot_cols]\n",
    "\n",
    "print(categorical_cols)\n",
    "print(numerical_cols)\n",
    "\n",
    "# Fill missing values if needed (optional but recommended)\n",
    "# X[categorical_cols] = X[categorical_cols].fillna('Missing')\n",
    "# X[numerical_cols] = X[numerical_cols].fillna(0)\n",
    "\n",
    "prefixes = {col: col[:3] for col in categorical_cols}\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "X = pd.get_dummies(X, columns=categorical_cols, prefix=(prefixes), dtype=int)\n",
    "\n",
    "# Scale numeric columns\n",
    "scaler = StandardScaler()\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344e126",
   "metadata": {},
   "source": [
    "put the data through Logistic regression model and report accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b056b805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of predicting training data = 0.9798900462962963\n",
      "Accuracy of predicting test data = 0.9774305555555556\n"
     ]
    }
   ],
   "source": [
    "model = ml.LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict training and test labels, and calculate accuracy\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print (f'Accuracy of predicting training data = {accuracy_train}')\n",
    "print (f'Accuracy of predicting test data = {accuracy_test}')\n",
    "\n",
    "# Examine feature weights and sort by most influential\n",
    "co_eff = model.coef_[0]\n",
    "\n",
    "co_eff_df = pd.DataFrame()\n",
    "co_eff_df['feature'] = list(X)\n",
    "co_eff_df['co_eff'] = co_eff\n",
    "co_eff_df['abs_co_eff'] = np.abs(co_eff)\n",
    "co_eff_df.sort_values(by='abs_co_eff', ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34376365",
   "metadata": {},
   "source": [
    "plot top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "366e2614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "co_eff_df.head(10).plot(kind='barh', x='feature', y='co_eff', legend=False)\n",
    "plt.title(\"Top 10 Influential Features\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12345e0",
   "metadata": {},
   "source": [
    "Use machine learning to run the data through many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f527af48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcdonaldh\\AppData\\Local\\anaconda3\\envs\\hsma_project\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 139, number of negative: 6773\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 74\n",
      "[LightGBM] [Info] Number of data points in the train set: 6912, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020110 -> initscore=-3.886225\n",
      "[LightGBM] [Info] Start training from score -3.886225\n",
      "                       Model  Training_accuracy  Test_accuracy  Precision  \\\n",
      "0    Decision Tree - Depth:1           0.979890       0.977431   0.977431   \n",
      "1    Random Forest - Depth:1           0.979890       0.977431   0.977431   \n",
      "2         XG Boost - Depth:1           0.979890       0.977431   0.977431   \n",
      "3    Decision Tree - Depth:2           0.979890       0.977431   0.977431   \n",
      "4    Random Forest - Depth:2           0.979890       0.977431   0.977431   \n",
      "5         XG Boost - Depth:2           0.979890       0.977431   0.977431   \n",
      "6    Decision Tree - Depth:3           0.979890       0.977431   0.977431   \n",
      "7    Random Forest - Depth:3           0.979890       0.977431   0.977431   \n",
      "8         XG Boost - Depth:3           0.979890       0.977431   0.977431   \n",
      "9    Decision Tree - Depth:4           0.979890       0.977431   0.977431   \n",
      "10   Random Forest - Depth:4           0.979890       0.977431   0.977431   \n",
      "11        XG Boost - Depth:4           0.979890       0.977431   0.977431   \n",
      "12   Decision Tree - Depth:5           0.979890       0.977431   0.977431   \n",
      "13   Random Forest - Depth:5           0.979890       0.977431   0.977431   \n",
      "14        XG Boost - Depth:5           0.979890       0.977431   0.977431   \n",
      "15   Decision Tree - Depth:6           0.980035       0.976852   0.976852   \n",
      "16   Random Forest - Depth:6           0.979890       0.977431   0.977431   \n",
      "17        XG Boost - Depth:6           0.979890       0.977431   0.977431   \n",
      "18   Decision Tree - Depth:7           0.980035       0.976852   0.976852   \n",
      "19   Random Forest - Depth:7           0.979890       0.977431   0.977431   \n",
      "20        XG Boost - Depth:7           0.979890       0.977431   0.977431   \n",
      "21   Decision Tree - Depth:8           0.980035       0.976852   0.976852   \n",
      "22   Random Forest - Depth:8           0.979890       0.977431   0.977431   \n",
      "23        XG Boost - Depth:8           0.979890       0.977431   0.977431   \n",
      "24   Decision Tree - Depth:9           0.980035       0.976852   0.976852   \n",
      "25   Random Forest - Depth:9           0.979890       0.977431   0.977431   \n",
      "26        XG Boost - Depth:9           0.979890       0.977431   0.977431   \n",
      "27       Logistic Regression           0.979890       0.977431   0.977431   \n",
      "28                 ADA Boost           0.979890       0.977431   0.977431   \n",
      "29                 Cat Boost           0.980035       0.976852   0.976852   \n",
      "30      Light Gradient Boost           0.979890       0.977431   0.977431   \n",
      "31  Histogram Gradient Boost           0.979890       0.977431   0.977431   \n",
      "32    Support Vector Machine           0.979890       0.977431   0.977431   \n",
      "33               Naive Bayes           0.099103       0.096644   0.096644   \n",
      "\n",
      "      Recall  Specificity  F1 Score  Training MAE  Testing MAE  Training MSE  \\\n",
      "0   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "1   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "2   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "3   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "4   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "5   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "6   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "7   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "8   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "9   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "10  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "11  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "12  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "13  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "14  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "15  0.976852     0.977417  0.976852      0.019965     0.023148      0.019965   \n",
      "16  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "17  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "18  0.976852     0.977417  0.976852      0.019965     0.023148      0.019965   \n",
      "19  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "20  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "21  0.976852     0.977417  0.976852      0.019965     0.023148      0.019965   \n",
      "22  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "23  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "24  0.976852     0.977417  0.976852      0.019965     0.023148      0.019965   \n",
      "25  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "26  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "27  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "28  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "29  0.976852     0.977417  0.976852      0.019965     0.023148      0.019965   \n",
      "30  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "31  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "32  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
      "33  0.096644     1.000000  0.096644      0.900897     0.903356      0.900897   \n",
      "\n",
      "    Test MSE  Training RMSE  Test RMSE  Training R2    Test R2  \n",
      "0   0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "1   0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "2   0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "3   0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "4   0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "5   0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "6   0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "7   0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "8   0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "9   0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "10  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "11  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "12  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "13  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "14  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "15  0.023148       0.141299   0.152145    -0.013181  -0.049324  \n",
      "16  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "17  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "18  0.023148       0.141299   0.152145    -0.013181  -0.049324  \n",
      "19  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "20  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "21  0.023148       0.141299   0.152145    -0.013181  -0.049324  \n",
      "22  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "23  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "24  0.023148       0.141299   0.152145    -0.013181  -0.049324  \n",
      "25  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "26  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "27  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "28  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "29  0.023148       0.141299   0.152145    -0.013181  -0.049324  \n",
      "30  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "31  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "32  0.022569       0.141810   0.150231    -0.020523  -0.023091  \n",
      "33  0.903356       0.949156   0.950451   -44.717947 -39.949857  \n"
     ]
    }
   ],
   "source": [
    "# 2. Run all models\n",
    "results_df = ml.run_all_models(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# 3. View results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53e5a99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training_accuracy</th>\n",
       "      <th>Test_accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Training MAE</th>\n",
       "      <th>Testing MAE</th>\n",
       "      <th>Training MSE</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Training RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>Training R Squared</th>\n",
       "      <th>Test R Squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree - Depth:1</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest - Depth:1</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XG Boost - Depth:1</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree - Depth:2</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest - Depth:2</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XG Boost - Depth:2</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree - Depth:3</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest - Depth:3</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XG Boost - Depth:3</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Decision Tree - Depth:4</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Random Forest - Depth:4</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XG Boost - Depth:4</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Decision Tree - Depth:5</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Random Forest - Depth:5</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XG Boost - Depth:5</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Decision Tree - Depth:6</td>\n",
       "      <td>0.980035</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.977417</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.019965</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.019965</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.141299</td>\n",
       "      <td>0.152145</td>\n",
       "      <td>-0.013181</td>\n",
       "      <td>-0.049324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Random Forest - Depth:6</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>XG Boost - Depth:6</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Decision Tree - Depth:7</td>\n",
       "      <td>0.980035</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.977417</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.019965</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.019965</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.141299</td>\n",
       "      <td>0.152145</td>\n",
       "      <td>-0.013181</td>\n",
       "      <td>-0.049324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Random Forest - Depth:7</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>XG Boost - Depth:7</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Decision Tree - Depth:8</td>\n",
       "      <td>0.980035</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.977417</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.019965</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.019965</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.141299</td>\n",
       "      <td>0.152145</td>\n",
       "      <td>-0.013181</td>\n",
       "      <td>-0.049324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Random Forest - Depth:8</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>XG Boost - Depth:8</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Decision Tree - Depth:9</td>\n",
       "      <td>0.980035</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.977417</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.019965</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.019965</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.141299</td>\n",
       "      <td>0.152145</td>\n",
       "      <td>-0.013181</td>\n",
       "      <td>-0.049324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Random Forest - Depth:9</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>XG Boost - Depth:9</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ADA Boost</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Cat Boost</td>\n",
       "      <td>0.980035</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.977417</td>\n",
       "      <td>0.976852</td>\n",
       "      <td>0.019965</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.019965</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.141299</td>\n",
       "      <td>0.152145</td>\n",
       "      <td>-0.013181</td>\n",
       "      <td>-0.049324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Light Gradient Boost</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Histogram Gradient Boost</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.979890</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.977431</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>-0.020523</td>\n",
       "      <td>-0.023091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.099103</td>\n",
       "      <td>0.096644</td>\n",
       "      <td>0.096644</td>\n",
       "      <td>0.096644</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.096644</td>\n",
       "      <td>0.900897</td>\n",
       "      <td>0.903356</td>\n",
       "      <td>0.900897</td>\n",
       "      <td>0.903356</td>\n",
       "      <td>0.949156</td>\n",
       "      <td>0.950451</td>\n",
       "      <td>-44.717947</td>\n",
       "      <td>-39.949857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  Training_accuracy  Test_accuracy  Precision  \\\n",
       "0    Decision Tree - Depth:1           0.979890       0.977431   0.977431   \n",
       "1    Random Forest - Depth:1           0.979890       0.977431   0.977431   \n",
       "2         XG Boost - Depth:1           0.979890       0.977431   0.977431   \n",
       "3    Decision Tree - Depth:2           0.979890       0.977431   0.977431   \n",
       "4    Random Forest - Depth:2           0.979890       0.977431   0.977431   \n",
       "5         XG Boost - Depth:2           0.979890       0.977431   0.977431   \n",
       "6    Decision Tree - Depth:3           0.979890       0.977431   0.977431   \n",
       "7    Random Forest - Depth:3           0.979890       0.977431   0.977431   \n",
       "8         XG Boost - Depth:3           0.979890       0.977431   0.977431   \n",
       "9    Decision Tree - Depth:4           0.979890       0.977431   0.977431   \n",
       "10   Random Forest - Depth:4           0.979890       0.977431   0.977431   \n",
       "11        XG Boost - Depth:4           0.979890       0.977431   0.977431   \n",
       "12   Decision Tree - Depth:5           0.979890       0.977431   0.977431   \n",
       "13   Random Forest - Depth:5           0.979890       0.977431   0.977431   \n",
       "14        XG Boost - Depth:5           0.979890       0.977431   0.977431   \n",
       "15   Decision Tree - Depth:6           0.980035       0.976852   0.976852   \n",
       "16   Random Forest - Depth:6           0.979890       0.977431   0.977431   \n",
       "17        XG Boost - Depth:6           0.979890       0.977431   0.977431   \n",
       "18   Decision Tree - Depth:7           0.980035       0.976852   0.976852   \n",
       "19   Random Forest - Depth:7           0.979890       0.977431   0.977431   \n",
       "20        XG Boost - Depth:7           0.979890       0.977431   0.977431   \n",
       "21   Decision Tree - Depth:8           0.980035       0.976852   0.976852   \n",
       "22   Random Forest - Depth:8           0.979890       0.977431   0.977431   \n",
       "23        XG Boost - Depth:8           0.979890       0.977431   0.977431   \n",
       "24   Decision Tree - Depth:9           0.980035       0.976852   0.976852   \n",
       "25   Random Forest - Depth:9           0.979890       0.977431   0.977431   \n",
       "26        XG Boost - Depth:9           0.979890       0.977431   0.977431   \n",
       "27       Logistic Regression           0.979890       0.977431   0.977431   \n",
       "28                 ADA Boost           0.979890       0.977431   0.977431   \n",
       "29                 Cat Boost           0.980035       0.976852   0.976852   \n",
       "30      Light Gradient Boost           0.979890       0.977431   0.977431   \n",
       "31  Histogram Gradient Boost           0.979890       0.977431   0.977431   \n",
       "32    Support Vector Machine           0.979890       0.977431   0.977431   \n",
       "33               Naive Bayes           0.099103       0.096644   0.096644   \n",
       "\n",
       "      Recall  Specificity  F1 Score  Training MAE  Testing MAE  Training MSE  \\\n",
       "0   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "1   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "2   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "3   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "4   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "5   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "6   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "7   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "8   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "9   0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "10  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "11  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "12  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "13  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "14  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "15  0.976852     0.977417  0.976852      0.019965     0.023148      0.019965   \n",
       "16  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "17  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "18  0.976852     0.977417  0.976852      0.019965     0.023148      0.019965   \n",
       "19  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "20  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "21  0.976852     0.977417  0.976852      0.019965     0.023148      0.019965   \n",
       "22  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "23  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "24  0.976852     0.977417  0.976852      0.019965     0.023148      0.019965   \n",
       "25  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "26  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "27  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "28  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "29  0.976852     0.977417  0.976852      0.019965     0.023148      0.019965   \n",
       "30  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "31  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "32  0.977431     0.977431  0.977431      0.020110     0.022569      0.020110   \n",
       "33  0.096644     1.000000  0.096644      0.900897     0.903356      0.900897   \n",
       "\n",
       "    Test MSE  Training RMSE  Test RMSE  Training R Squared  Test R Squared  \n",
       "0   0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "1   0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "2   0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "3   0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "4   0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "5   0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "6   0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "7   0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "8   0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "9   0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "10  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "11  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "12  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "13  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "14  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "15  0.023148       0.141299   0.152145           -0.013181       -0.049324  \n",
       "16  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "17  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "18  0.023148       0.141299   0.152145           -0.013181       -0.049324  \n",
       "19  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "20  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "21  0.023148       0.141299   0.152145           -0.013181       -0.049324  \n",
       "22  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "23  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "24  0.023148       0.141299   0.152145           -0.013181       -0.049324  \n",
       "25  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "26  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "27  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "28  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "29  0.023148       0.141299   0.152145           -0.013181       -0.049324  \n",
       "30  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "31  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "32  0.022569       0.141810   0.150231           -0.020523       -0.023091  \n",
       "33  0.903356       0.949156   0.950451          -44.717947      -39.949857  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results_df)\n",
    "\n",
    "#Label columns\n",
    "results_df.columns = ['Model', 'Training_accuracy', 'Test_accuracy', 'Precision',\n",
    "                      'Recall', 'Specificity', 'F1 Score', 'Training MAE',\n",
    "                      'Testing MAE', 'Training MSE', 'Test MSE', 'Training RMSE',\n",
    "                      'Test RMSE', 'Training R Squared', 'Test R Squared']\n",
    "\n",
    "# # Save to CSV\n",
    "results_df.to_csv('results_many_models.csv', index=False)\n",
    "\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsma_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
